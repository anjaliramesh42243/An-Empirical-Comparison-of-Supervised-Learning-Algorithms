{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A Final Project: Model Comparison between SVM, Logistic Regression, and K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anjali Ramesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os \n",
    "import random\n",
    "import sklearn\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import patsy\n",
    "#import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df = pd.read_csv(\"Data/adult.data\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>final_weight</th>\n",
       "      <th>education</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  final_weight   education  education_num  \\\n",
       "0   39          State-gov         77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc         83311   Bachelors             13   \n",
       "2   38            Private        215646     HS-grad              9   \n",
       "3   53            Private        234721        11th              7   \n",
       "4   28            Private        338409   Bachelors             13   \n",
       "\n",
       "        marital_status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital_gain  capital_loss  hours_per_week  native_country  income  \n",
       "0          2174             0              40   United-States   <=50K  \n",
       "1             0             0              13   United-States   <=50K  \n",
       "2             0             0              40   United-States   <=50K  \n",
       "3             0             0              40   United-States   <=50K  \n",
       "4             0             0              40            Cuba   <=50K  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming columns\n",
    "adult_df.columns = [\"age\", \"workclass\", \"final_weight\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\"]\n",
    "\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21790\n",
       "1    10771\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary classification of income and sex\n",
    "adult_df[\"income\"] = adult_df[\"income\"].apply(lambda x: 1  if x == \" >50K\" else 0)\n",
    "\n",
    "# adult_df[\"income\"].value_counts()\n",
    "\n",
    "adult_df[\"sex\"] = adult_df[\"sex\"].apply(lambda x: 1  if x == \" Female\" else 0)\n",
    "adult_df[\"sex\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding of nominal values\n",
    "\n",
    "workclass_dummies = pd.get_dummies(adult_df.workclass, prefix='workclass')\n",
    "adult_df = pd.concat([adult_df, workclass_dummies], axis=1)\n",
    "\n",
    "education_dummies = pd.get_dummies(adult_df.education, prefix='education')\n",
    "adult_df = pd.concat([adult_df, education_dummies], axis=1)\n",
    "\n",
    "marital_status_dummies = pd.get_dummies(adult_df.marital_status, prefix='marital_status')\n",
    "adult_df = pd.concat([adult_df, marital_status_dummies], axis=1)\n",
    "\n",
    "occupation_dummies = pd.get_dummies(adult_df.occupation, prefix='occupation')\n",
    "adult_df = pd.concat([adult_df, occupation_dummies], axis=1)\n",
    "\n",
    "relationship_dummies = pd.get_dummies(adult_df.relationship, prefix='race')\n",
    "adult_df = pd.concat([adult_df, relationship_dummies], axis=1)\n",
    "\n",
    "race_dummies = pd.get_dummies(adult_df.race, prefix='race')\n",
    "adult_df = pd.concat([adult_df, race_dummies], axis=1)\n",
    "\n",
    "native_country_dummies = pd.get_dummies(adult_df.native_country, prefix='native_country')\n",
    "adult_df = pd.concat([adult_df, native_country_dummies], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 115)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>final_weight</th>\n",
       "      <th>education_num</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>income</th>\n",
       "      <th>workclass_ ?</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>native_country_ Portugal</th>\n",
       "      <th>native_country_ Puerto-Rico</th>\n",
       "      <th>native_country_ Scotland</th>\n",
       "      <th>native_country_ South</th>\n",
       "      <th>native_country_ Taiwan</th>\n",
       "      <th>native_country_ Thailand</th>\n",
       "      <th>native_country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native_country_ United-States</th>\n",
       "      <th>native_country_ Vietnam</th>\n",
       "      <th>native_country_ Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  final_weight  education_num  sex  capital_gain  capital_loss  \\\n",
       "0   39         77516             13    0          2174             0   \n",
       "1   50         83311             13    0             0             0   \n",
       "2   38        215646              9    0             0             0   \n",
       "3   53        234721              7    0             0             0   \n",
       "4   28        338409             13    1             0             0   \n",
       "\n",
       "   hours_per_week  income  workclass_ ?  workclass_ Federal-gov  ...  \\\n",
       "0              40       0             0                       0  ...   \n",
       "1              13       0             0                       0  ...   \n",
       "2              40       0             0                       0  ...   \n",
       "3              40       0             0                       0  ...   \n",
       "4              40       0             0                       0  ...   \n",
       "\n",
       "   native_country_ Portugal  native_country_ Puerto-Rico  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "2                         0                            0   \n",
       "3                         0                            0   \n",
       "4                         0                            0   \n",
       "\n",
       "   native_country_ Scotland  native_country_ South  native_country_ Taiwan  \\\n",
       "0                         0                      0                       0   \n",
       "1                         0                      0                       0   \n",
       "2                         0                      0                       0   \n",
       "3                         0                      0                       0   \n",
       "4                         0                      0                       0   \n",
       "\n",
       "   native_country_ Thailand  native_country_ Trinadad&Tobago  \\\n",
       "0                         0                                0   \n",
       "1                         0                                0   \n",
       "2                         0                                0   \n",
       "3                         0                                0   \n",
       "4                         0                                0   \n",
       "\n",
       "   native_country_ United-States  native_country_ Vietnam  \\\n",
       "0                              1                        0   \n",
       "1                              1                        0   \n",
       "2                              1                        0   \n",
       "3                              1                        0   \n",
       "4                              0                        0   \n",
       "\n",
       "   native_country_ Yugoslavia  \n",
       "0                           0  \n",
       "1                           0  \n",
       "2                           0  \n",
       "3                           0  \n",
       "4                           0  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adult_df = adult_df.drop( columns=['workclass', 'education', 'marital_status', 'occupation', \n",
    "                                   'relationship', \"race\", \"native_country\"])\n",
    "\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>final_weight</th>\n",
       "      <th>education_num</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital_gain</th>\n",
       "      <th>capital_loss</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>workclass_ ?</th>\n",
       "      <th>workclass_ Federal-gov</th>\n",
       "      <th>workclass_ Local-gov</th>\n",
       "      <th>...</th>\n",
       "      <th>native_country_ Puerto-Rico</th>\n",
       "      <th>native_country_ Scotland</th>\n",
       "      <th>native_country_ South</th>\n",
       "      <th>native_country_ Taiwan</th>\n",
       "      <th>native_country_ Thailand</th>\n",
       "      <th>native_country_ Trinadad&amp;Tobago</th>\n",
       "      <th>native_country_ United-States</th>\n",
       "      <th>native_country_ Vietnam</th>\n",
       "      <th>native_country_ Yugoslavia</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  final_weight  education_num  sex  capital_gain  capital_loss  \\\n",
       "0   39         77516             13    0          2174             0   \n",
       "1   50         83311             13    0             0             0   \n",
       "2   38        215646              9    0             0             0   \n",
       "3   53        234721              7    0             0             0   \n",
       "4   28        338409             13    1             0             0   \n",
       "\n",
       "   hours_per_week  workclass_ ?  workclass_ Federal-gov  workclass_ Local-gov  \\\n",
       "0              40             0                       0                     0   \n",
       "1              13             0                       0                     0   \n",
       "2              40             0                       0                     0   \n",
       "3              40             0                       0                     0   \n",
       "4              40             0                       0                     0   \n",
       "\n",
       "   ...  native_country_ Puerto-Rico  native_country_ Scotland  \\\n",
       "0  ...                            0                         0   \n",
       "1  ...                            0                         0   \n",
       "2  ...                            0                         0   \n",
       "3  ...                            0                         0   \n",
       "4  ...                            0                         0   \n",
       "\n",
       "   native_country_ South  native_country_ Taiwan  native_country_ Thailand  \\\n",
       "0                      0                       0                         0   \n",
       "1                      0                       0                         0   \n",
       "2                      0                       0                         0   \n",
       "3                      0                       0                         0   \n",
       "4                      0                       0                         0   \n",
       "\n",
       "   native_country_ Trinadad&Tobago  native_country_ United-States  \\\n",
       "0                                0                              1   \n",
       "1                                0                              1   \n",
       "2                                0                              1   \n",
       "3                                0                              1   \n",
       "4                                0                              0   \n",
       "\n",
       "   native_country_ Vietnam  native_country_ Yugoslavia  income  \n",
       "0                        0                           0       0  \n",
       "1                        0                           0       0  \n",
       "2                        0                           0       0  \n",
       "3                        0                           0       0  \n",
       "4                        0                           0       0  \n",
       "\n",
       "[5 rows x 108 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(adult_df.columns.values) #Make a list of all of the columns in the df\n",
    "\n",
    "cols.pop(cols.index('income')) #Remove income from list\n",
    "\n",
    "adult_df = adult_df[cols + ['income']] #Create new dataframe with income column at the end\n",
    "\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_df = pd.read_csv(\"Data/letter-recognition.data\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "letter_df.columns = [\"cap_letter\", \"xboxh\", \"xboxy\", \"width\", \"height\", \n",
    "                     \"pix\", \"xmean_pix\", \"y_meanpix\", \"x2_var\", \"y2_var\", \n",
    "                     \"xybar\", \"x2ybr\", \"xy2br\", \"x_ege\", \"xegvy\", \"y_ege\", \"yegvx\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10060\n",
       "1     9940\n",
       "Name: cap_letter, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#binary classification of capital letter columns (A-M positive, else negative)\n",
    "alphabet_array = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M']\n",
    "\n",
    "letter_df[\"cap_letter\"] = letter_df[\"cap_letter\"].apply(lambda x: 1  if x in alphabet_array else 0)\n",
    "\n",
    "letter_df['cap_letter'].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cap_letter</th>\n",
       "      <th>xboxh</th>\n",
       "      <th>xboxy</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>pix</th>\n",
       "      <th>xmean_pix</th>\n",
       "      <th>y_meanpix</th>\n",
       "      <th>x2_var</th>\n",
       "      <th>y2_var</th>\n",
       "      <th>xybar</th>\n",
       "      <th>x2ybr</th>\n",
       "      <th>xy2br</th>\n",
       "      <th>x_ege</th>\n",
       "      <th>xegvy</th>\n",
       "      <th>y_ege</th>\n",
       "      <th>yegvx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cap_letter  xboxh  xboxy  width  height  pix  xmean_pix  y_meanpix  x2_var  \\\n",
       "0           0      2      8      3       5    1          8         13       0   \n",
       "1           1      5     12      3       7    2         10          5       5   \n",
       "2           1      4     11      6       8    6         10          6       2   \n",
       "3           0      7     11      6       6    3          5          9       4   \n",
       "4           1      2      1      3       1    1          8          6       6   \n",
       "\n",
       "   y2_var  xybar  x2ybr  xy2br  x_ege  xegvy  y_ege  yegvx  \n",
       "0       6      6     10      8      0      8      0      8  \n",
       "1       4     13      3      9      2      8      4     10  \n",
       "2       6     10      3      7      3      7      3      9  \n",
       "3       6      4      4     10      6     10      2      8  \n",
       "4       6      6      5      9      1      7      5     10  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic Telescope Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_df = pd.read_csv(\"Data/magic04.data\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1       2       3       4         5        6        7   \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110  -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238  -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580 -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633  -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525  21.8393   \n",
       "\n",
       "        8         9  10  \n",
       "0  40.0920   81.8828  g  \n",
       "1   6.3609  205.2610  g  \n",
       "2  76.9600  256.7880  g  \n",
       "3  10.4490  116.7370  g  \n",
       "4   4.6480  356.4620  g  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "magic_df.columns = [\"fLength\", \"fWidth\", \"fSize\", \"fConc\", \"fConc1\", \"fAsym\", \"fM3Long\", \"fM3Trans\", \"fAlpha\", \"fDist\", \"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g    12332\n",
       "h     6688\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magic_df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12332\n",
       "0     6688\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#binary classification of capital letter columns (A-M positive, else negative)\n",
    "magic_df[\"class\"] = magic_df[\"class\"].apply(lambda x: 1  if x == 'g' else 0)\n",
    "\n",
    "magic_df[\"class\"].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM3Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM3Long  fM3Trans  \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
       "\n",
       "    fAlpha     fDist  class  \n",
       "0  40.0920   81.8828      1  \n",
       "1   6.3609  205.2610      1  \n",
       "2  76.9600  256.7880      1  \n",
       "3  10.4490  116.7370      1  \n",
       "4   4.6480  356.4620      1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "magic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVTYPE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cover_df = pd.read_csv(\"Data/covtype.data\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elevation</th>\n",
       "      <th>aspect</th>\n",
       "      <th>slope</th>\n",
       "      <th>horiz_distance</th>\n",
       "      <th>vert_distance</th>\n",
       "      <th>dist_roadway</th>\n",
       "      <th>shade_9</th>\n",
       "      <th>shade_12</th>\n",
       "      <th>shade_3</th>\n",
       "      <th>dist_to_fire</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>cover_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2596</td>\n",
       "      <td>51</td>\n",
       "      <td>3</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>510</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>148</td>\n",
       "      <td>6279</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   elevation  aspect  slope  horiz_distance  vert_distance  dist_roadway  \\\n",
       "0       2596      51      3             258              0           510   \n",
       "1       2590      56      2             212             -6           390   \n",
       "2       2804     139      9             268             65          3180   \n",
       "3       2785     155     18             242            118          3090   \n",
       "4       2595      45      2             153             -1           391   \n",
       "\n",
       "   shade_9  shade_12  shade_3  dist_to_fire  ...  45  46  47  48  49  50  51  \\\n",
       "0      221       232      148          6279  ...   0   0   0   0   0   0   0   \n",
       "1      220       235      151          6225  ...   0   0   0   0   0   0   0   \n",
       "2      234       238      135          6121  ...   0   0   0   0   0   0   0   \n",
       "3      238       238      122          6211  ...   0   0   0   0   0   0   0   \n",
       "4      220       234      150          6172  ...   0   0   0   0   0   0   0   \n",
       "\n",
       "   52  53  cover_type  \n",
       "0   0   0           5  \n",
       "1   0   0           5  \n",
       "2   0   0           2  \n",
       "3   0   0           2  \n",
       "4   0   0           5  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rename columns\n",
    "cover_df = cover_df.rename(columns={ 0: \"elevation\", 1: \"aspect\", 2: \"slope\", 3 :\"horiz_distance\", \n",
    "                                    4: \"vert_distance\", 5: \"dist_roadway\", 6: \"shade_9\", 7: \"shade_12\", \n",
    "                                    8: \"shade_3\" , 9: \"dist_to_fire\", 54: \"cover_type\"})\n",
    "cover_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    283301\n",
       "1    211840\n",
       "3     35754\n",
       "7     20510\n",
       "6     17367\n",
       "5      9493\n",
       "4      2747\n",
       "Name: cover_type, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which cover type class is the largest\n",
    "cover_df[\"cover_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    297711\n",
       "1    283301\n",
       "Name: cover_type, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#binary classification of cover type column, largest class is positive\n",
    "\n",
    "cover_df[\"cover_type\"] = cover_df[\"cover_type\"].apply(lambda x: 1  if x == 2 else 0)\n",
    "cover_df[\"cover_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "calhous_df = pd.read_csv(\"Data/cal_housing.data\", header = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>med_age</th>\n",
       "      <th>tot_rooms</th>\n",
       "      <th>tot_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>med_income</th>\n",
       "      <th>med_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  med_age  tot_rooms  tot_bedrooms  population  \\\n",
       "0    -122.23     37.88     41.0      880.0         129.0       322.0   \n",
       "1    -122.22     37.86     21.0     7099.0        1106.0      2401.0   \n",
       "2    -122.24     37.85     52.0     1467.0         190.0       496.0   \n",
       "3    -122.25     37.85     52.0     1274.0         235.0       558.0   \n",
       "4    -122.25     37.85     52.0     1627.0         280.0       565.0   \n",
       "\n",
       "   households  med_income  med_house_value  \n",
       "0       126.0      8.3252         452600.0  \n",
       "1      1138.0      8.3014         358500.0  \n",
       "2       177.0      7.2574         352100.0  \n",
       "3       219.0      5.6431         341300.0  \n",
       "4       259.0      3.8462         342200.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renaming columns\n",
    "calhous_df.columns = [\"longitude\", \"latitude\", \"med_age\", \"tot_rooms\", \"tot_bedrooms\", \n",
    "                      \"population\", \"households\", \"med_income\", \"med_house_value\"]\n",
    "\n",
    "calhous_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14728\n",
       "0     5912\n",
       "Name: med_house_value, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#binary classification of median house value, > $130000 belongs to positive class\n",
    "\n",
    "calhous_df[\"med_house_value\"] = calhous_df[\"med_house_value\"].apply(lambda x: 1  if x > 130000 else 0)\n",
    "calhous_df[\"med_house_value\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>med_age</th>\n",
       "      <th>tot_rooms</th>\n",
       "      <th>tot_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>med_income</th>\n",
       "      <th>med_house_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  med_age  tot_rooms  tot_bedrooms  population  \\\n",
       "0    -122.23     37.88     41.0      880.0         129.0       322.0   \n",
       "1    -122.22     37.86     21.0     7099.0        1106.0      2401.0   \n",
       "2    -122.24     37.85     52.0     1467.0         190.0       496.0   \n",
       "3    -122.25     37.85     52.0     1274.0         235.0       558.0   \n",
       "4    -122.25     37.85     52.0     1627.0         280.0       565.0   \n",
       "\n",
       "   households  med_income  med_house_value  \n",
       "0       126.0      8.3252                1  \n",
       "1      1138.0      8.3014                1  \n",
       "2       177.0      7.2574                1  \n",
       "3       219.0      5.6431                1  \n",
       "4       259.0      3.8462                1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calhous_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting datasets into X columns and Y column (classification)\n",
    "\n",
    "# X_adult = adult_df.iloc[:, :-1]\n",
    "# y_adult = adult_df.iloc[:, -1:]\n",
    "\n",
    "X_cover = cover_df.iloc[:, :-1]\n",
    "y_cover = cover_df.iloc[:, -1:]\n",
    "\n",
    "\n",
    "X_calhous = calhous_df.iloc[:, :-1]\n",
    "y_calhous = calhous_df.iloc[:, -1:]\n",
    "\n",
    "X_magic = magic_df.iloc[:, :-1]\n",
    "y_magic = magic_df.iloc[:, -1:]\n",
    "\n",
    "\n",
    "X_letter = letter_df.iloc[:, 1:]\n",
    "y_letter = letter_df.iloc[:, 0]\n",
    "\n",
    "#list to loop through in each model\n",
    "all_X = [X_magic, X_letter, X_calhous, X_cover]\n",
    "all_y = [y_magic, y_letter, y_calhous,y_cover]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.738625444377988, 0.7212302277607805, 0.7149038581937766, 0.6926875834826314, 0.7372923862455938]\n",
      "[0.6477888730385164, 0.6496433666191156, 0.6497146932952924, 0.6516405135520684, 0.6542082738944365]\n",
      "[0.7851922742300331, 0.7866759315556328, 0.7867841792211175, 0.7882046834345187, 0.7900207900207901]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.9879579498422674, 0.9873618095423848, 0.9880558178240166, 0.9879242126078567, 0.9882366026935595]\n",
      "[0.8937333333333334, 0.8526666666666667, 0.8681333333333333, 0.8466, 0.8724666666666666]\n",
      "[0.8819084308786487, 0.8276668746101061, 0.8484523444682808, 0.8203606838941367, 0.8544915189777136]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.49379409423676207, 0.4957741261602221, 0.4955304961912253, 0.49476372850374023, 0.4978670845374629]\n",
      "[0.7156010230179028, 0.7148337595907929, 0.7126598465473146, 0.7131713554987212, 0.7126598465473146]\n",
      "[0.8342277877161598, 0.8337061894108874, 0.8322257895915777, 0.8325744569679779, 0.8322257895915777]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.44312339808372886, 0.45211078911828395, 0.49731356030423784, 0.4972648261610282, 0.4974118645777465]\n",
      "[0.5123539092935564, 0.5123452289188419, 0.5124268244411575, 0.5123799504176997, 0.5123973111671285]\n",
      "[0.0, 0.0, 7.121239095602634e-06, 2.8481913984619766e-05, 1.424151472750642e-05]\n",
      "SVM test averages  [[7.20947900e-01 6.50599144e-01 7.87375572e-01]\n",
      " [9.87907279e-01 8.66720000e-01 8.46575971e-01]\n",
      " [4.95545906e-01 7.13785166e-01 8.32992003e-01]\n",
      " [4.77444888e-01 5.12380645e-01 9.96893356e-06]]\n",
      "SVM train averages  [[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 1. 1.]\n",
      " [0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# empty lists to append metrics per trial\n",
    "test_svm_metrics = []\n",
    "train_svm_metrics = []\n",
    "\n",
    "#iterating through each of the 4 datasets\n",
    "for X, y in zip(all_X, all_y):\n",
    "    auc_test_array = []\n",
    "    acc_test_array = []\n",
    "    fsc_test_array = []\n",
    "\n",
    "    auc_train_array = []\n",
    "    acc_train_array = []\n",
    "    fsc_train_array = []\n",
    "    \n",
    "    for trial in range(5):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 5000)\n",
    "        pipe = Pipeline([('std', StandardScaler()),\n",
    "                         ('classifier', SVC(probability = True))])\n",
    "        search_space = [{'classifier': [SVC(probability = True)],\n",
    "                         'classifier__kernel': ['linear'],\n",
    "                        'classifier__C':[10**-1, 1, 10, 10**2]},\n",
    "                        {'classifier': [SVC(probability = True)],\n",
    "                         'classifier__kernel': ['poly'],\n",
    "                         'classifier__degree': [2,3],\n",
    "                        'classifier__C': [10**-1, 1, 10, 10**2]},\n",
    "                        {'classifier': [SVC(probability = True)],\n",
    "                         'classifier__kernel': ['rbf'],\n",
    "                         'classifier__gamma': [0.01,0.05,0.1,0.5,1,2],\n",
    "                        'classifier__C': [10**-1, 1, 10, 10**2]}\n",
    "                       ]\n",
    "\n",
    "        # Create grid search \n",
    "        clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                           scoring=[\"accuracy\",\"roc_auc\", \"f1\"], refit=False)\n",
    "\n",
    "        # Fit grid search\n",
    "        search = clf.fit(X_train, y_train)\n",
    "\n",
    "        auc_rank = search.cv_results_[\"rank_test_roc_auc\"]\n",
    "        acc_rank = search.cv_results_[\"rank_test_accuracy\"]\n",
    "        f1_rank = search.cv_results_[\"rank_test_f1\"]\n",
    "\n",
    "        ind_auc = list(auc_rank).index(min(auc_rank))\n",
    "        ind_acc = list(acc_rank).index(min(acc_rank))\n",
    "        ind_f1 = list(f1_rank).index(min(f1_rank))\n",
    "\n",
    "        opt_auc = search.cv_results_[\"params\"][ind_auc]\n",
    "        opt_acc = search.cv_results_[\"params\"][ind_acc]\n",
    "        opt_f1 = search.cv_results_[\"params\"][ind_f1]  \n",
    "\n",
    "        print(\"Trial \", trial)\n",
    "\n",
    "        #Model 1 - optimal model for AUC\n",
    "        if (opt_auc[\"classifier__kernel\"] == \"linear\"): #checking which kernel is optimal, choosing hyperparameters for chosen kernel\n",
    "            auc_model = SVC(kernel = \"linear\", C = opt_auc[\"classifier__C\"], probability = True)\n",
    "        elif (opt_auc[\"classifier__kernel\"] == \"poly\"):\n",
    "            auc_model = SVC(kernel = \"poly\", C = opt_auc[\"classifier__C\"], degree = opt_auc[\"classifier__degree\"], probability = True)\n",
    "        else:\n",
    "            auc_model = SVC(kernel = \"rbf\", C = opt_auc[\"classifier__C\"], gamma = opt_auc[\"classifier__gamma\"], probability = True)\n",
    "        auc_model.fit(X_train, y_train)\n",
    "\n",
    "        #predicting on train data\n",
    "        y_pred_train = auc_model.predict_proba(X_train)[:,1]\n",
    "        auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "        auc_train_array.append(auc_train) # appending optimal parameter train metric\n",
    "\n",
    "        #predicting on test data\n",
    "        y_pred_test = auc_model.predict_proba(X_test)[:,1]\n",
    "        auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "        auc_test_array.append(auc_test) # appending optimal parameter test metric\n",
    "\n",
    "        #Model 2 - optimal model for ACC\n",
    "        if (opt_acc[\"classifier__kernel\"] == \"linear\"):\n",
    "            acc_model = SVC(kernel = \"linear\", C = opt_acc[\"classifier__C\"], probability = True)\n",
    "        elif (opt_acc[\"classifier__kernel\"] == \"poly\"):\n",
    "            acc_model = SVC(kernel = \"poly\", C = opt_acc[\"classifier__C\"], degree = opt_acc[\"classifier__degree\"], probability = True)\n",
    "        else:\n",
    "            acc_model = SVC(kernel = \"rbf\", C = opt_acc[\"classifier__C\"], gamma = opt_acc[\"classifier__gamma\"], probability = True)\n",
    "        acc_model.fit(X_train, y_train)\n",
    "\n",
    "        #predicting on train data\n",
    "        acc_train = acc_model.score(X_train, y_train)\n",
    "        acc_train_array.append(acc_train)\n",
    "\n",
    "        #predicting on test data\n",
    "        acc_test = acc_model.score(X_test, y_test)\n",
    "        acc_test_array.append(acc_test)\n",
    "\n",
    "        #Model 3 - optimal model for FSC\n",
    "        if (opt_f1[\"classifier__kernel\"] == \"linear\"):\n",
    "            f1_model = SVC(kernel = \"linear\", C = opt_f1[\"classifier__C\"], probability = True)\n",
    "        elif (opt_f1[\"classifier__kernel\"] == \"poly\"):\n",
    "            f1_model = SVC(kernel = \"poly\", C = opt_f1[\"classifier__C\"], degree = opt_f1[\"classifier__degree\"], probability = True)\n",
    "        else:\n",
    "            f1_model = SVC(kernel = \"rbf\", C = opt_f1[\"classifier__C\"], gamma = opt_f1[\"classifier__gamma\"], probability = True)\n",
    "\n",
    "        f1_model.fit(X_train, y_train)\n",
    "\n",
    "        #predicting on train data\n",
    "        y_pred_train = f1_model.predict(X_train)\n",
    "        f1_train = f1_score(y_train, y_pred_train)\n",
    "        fsc_train_array.append(f1_train)\n",
    "\n",
    "        #predicting on test data\n",
    "        y_pred_test = f1_model.predict(X_test)\n",
    "        f1_test = f1_score(y_test, y_pred_test)\n",
    "        fsc_test_array.append(f1_test)\n",
    "\n",
    "\n",
    "    #take mean of 5 trials for train\n",
    "    auc_score_train = np.mean(auc_train_array)\n",
    "    acc_score_train = np.mean(acc_train_array)\n",
    "    fsc_score_train = np.mean(fsc_train_array)\n",
    "\n",
    "    #print raw test values\n",
    "    print(auc_test_array)\n",
    "    print(acc_test_array)\n",
    "    print(fsc_test_array)\n",
    "    #take mean of 5 trials for train\n",
    "    auc_score_test = np.mean(auc_test_array)\n",
    "    acc_score_test = np.mean(acc_test_array)\n",
    "    fsc_score_test = np.mean(fsc_test_array)\n",
    "\n",
    "    #append each dataset's metrics to new list\n",
    "    metrics_test = [auc_score_test, acc_score_test, fsc_score_test]\n",
    "    test_svm_metrics.append(metrics_test)\n",
    "\n",
    "    metrics_train = [auc_score_train, acc_score_train, fsc_score_train]\n",
    "    train_svm_metrics.append(metrics_train)\n",
    "    \n",
    "#4x3 (datasets x metrics) array of optimal errors for each dataset\n",
    "test_svm_metrics = np.array(test_svm_metrics)\n",
    "print(\"SVM test averages \", test_svm_metrics)\n",
    "        \n",
    "train_svm_metrics = np.array(train_svm_metrics)\n",
    "print(\"SVM train averages \", train_svm_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM test metrics across datasets:  [0.67046149 0.68587124 0.61673838]\n",
      "SVM train metrics across datasets:  [0.5 1.  1. ]\n",
      "SVM test metrics across metrics for each algo:  [0.71964087 0.90040108 0.68077436 0.32994517]\n"
     ]
    }
   ],
   "source": [
    "#Table 2 values\n",
    "#train and test metrics averaged across datasets\n",
    "train_svm_average_datasets = np.mean(train_svm_metrics, axis = 0)\n",
    "test_svm_average_datasets = np.mean(test_svm_metrics, axis = 0)\n",
    "\n",
    "#Table 3 values\n",
    "#test metrics averaged across metrics per dataset\n",
    "test_svm_average_metrics = np.mean(test_svm_metrics, axis = 1)\n",
    "\n",
    "\n",
    "print(\"SVM test metrics across datasets: \", test_svm_average_datasets)\n",
    "print(\"SVM train metrics across datasets: \", train_svm_average_datasets)\n",
    "\n",
    "print(\"SVM test metrics across metrics for each algo: \", test_svm_average_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.8321061493776546, 0.8315238704201514, 0.8231736539854053, 0.8366498019630457, 0.8276991965257093]\n",
      "[0.787660485021398, 0.7875891583452211, 0.7911554921540657, 0.7904422253922967, 0.7904422253922967]\n",
      "[0.8457296015669296, 0.8459548934409271, 0.8491390388075044, 0.8481810665564283, 0.848712667353244]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.8119508847865295, 0.8120677260149121, 0.812973775272348, 0.8130235424106899, 0.8092622735629882]\n",
      "[0.7262666666666666, 0.7274666666666667, 0.7252, 0.7320666666666666, 0.72]\n",
      "[0.7297973150829166, 0.7307337636675011, 0.7278489370130728, 0.737714546759773, 0.7189131307723197]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.8681977396810866, 0.8659182435673833, 0.8656068600982441, 0.8871206450435931, 0.8691265841013824]\n",
      "[0.8128516624040921, 0.8081841432225064, 0.8090153452685422, 0.8207161125319693, 0.8132992327365729]\n",
      "[0.8756320373911195, 0.8720354888244328, 0.8718520743060622, 0.8855297157622738, 0.8751389720345506]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.6593358803066984, 0.6590794338468785, 0.6661648174287814, 0.6590205360056959, 0.6576463451966736]\n",
      "[0.6123431456289105, 0.6098900717346166, 0.618546141399832, 0.6127181378165732, 0.6166364589626605]\n",
      "[0.590328753401008, 0.5857794884273085, 0.5874538198720382, 0.5762265606151848, 0.5944097302579135]\n",
      "LR test averages  [[0.83023053 0.78945792 0.84754345]\n",
      " [0.81185564 0.7262     0.72900154]\n",
      " [0.87119401 0.8128133  0.87603766]\n",
      " [0.6602494  0.61402679 0.58683967]]\n",
      "LR train averages  [[0.83108784 0.79168    0.84921259]\n",
      " [0.81549944 0.72652    0.72920203]\n",
      " [0.87315372 0.8124     0.875031  ]\n",
      " [0.66523399 0.61976    0.59217438]]\n"
     ]
    }
   ],
   "source": [
    "# empty lists to append metrics per trial\n",
    "test_logreg_metrics = []\n",
    "train_logreg_metrics = []\n",
    "\n",
    "#looping through all four datasets\n",
    "for X, y in zip(all_X, all_y):\n",
    "    auc_test_array = []\n",
    "    acc_test_array = []\n",
    "    fsc_test_array = []\n",
    "    \n",
    "    auc_train_array = []\n",
    "    acc_train_array = []\n",
    "    fsc_train_array = []\n",
    "    for trial in range(5):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 5000)\n",
    "        pipe = Pipeline([('std', StandardScaler()),\n",
    "                         ('classifier', LogisticRegression())])\n",
    "\n",
    "        # Create search space of candidate learning algorithms and their hyperparameters\n",
    "        # note lbfgs can't do l1, and if you pass penalty='none' it expects no C value\n",
    "        search_space = [{'classifier': [LogisticRegression()],\n",
    "                         'classifier__penalty': ['l2'],\n",
    "                         'classifier__C': [10**(-8), 10**(-7), 10**(-6), 10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 1, 10, 10**2, 10**3, 10**4],\n",
    "                        },\n",
    "\n",
    "                        {'classifier': [LogisticRegression()],\n",
    "                        'classifier__penalty': ['none']\n",
    "                        }]\n",
    "\n",
    "        # Create grid search \n",
    "        clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                           scoring=[\"accuracy\",\"roc_auc\", \"f1\"], refit=False)\n",
    "\n",
    "        # Fit grid search\n",
    "        search = clf.fit(X_train, y_train)\n",
    "        \n",
    "        #finding index of the best ranking mean test metrics\n",
    "        auc_rank = search.cv_results_[\"rank_test_roc_auc\"]\n",
    "        acc_rank = search.cv_results_[\"rank_test_accuracy\"]\n",
    "        f1_rank = search.cv_results_[\"rank_test_f1\"]\n",
    "\n",
    "        ind_auc = list(auc_rank).index(min(auc_rank))\n",
    "        ind_acc = list(acc_rank).index(min(acc_rank))\n",
    "        ind_f1 = list(f1_rank).index(min(f1_rank))\n",
    "\n",
    "\n",
    "        #storing optimal parameters as a dictionary\n",
    "        opt_auc = search.cv_results_[\"params\"][ind_auc]\n",
    "        opt_acc = search.cv_results_[\"params\"][ind_acc]\n",
    "        opt_f1 = search.cv_results_[\"params\"][ind_f1]    \n",
    "\n",
    "        print(\"Trial \", trial)\n",
    "        \n",
    "        #Model 1 - optimal model for AUC\n",
    "        auc_model = LogisticRegression(penalty = opt_auc[\"classifier__penalty\"], C = opt_auc[\"classifier__C\"])\n",
    "        auc_model.fit(X_train, y_train)\n",
    "        \n",
    "        #predicting on train data\n",
    "        y_pred_train = auc_model.predict_proba(X_train)[:,1]\n",
    "        auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "        auc_train_array.append(auc_train) # appending optimal parameter metric for train\n",
    "        \n",
    "        #predicting on test data\n",
    "        y_pred_test = auc_model.predict_proba(X_test)[:,1]\n",
    "        auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "        auc_test_array.append(auc_test) # appending optimal parameter metric for test\n",
    "        \n",
    "\n",
    "        #Model 2 - optimal model for ACC\n",
    "        acc_model = LogisticRegression(penalty = opt_acc[\"classifier__penalty\"], C = opt_acc[\"classifier__C\"])\n",
    "        acc_model.fit(X_train, y_train)\n",
    "        \n",
    "        acc_train = acc_model.score(X_train, y_train)\n",
    "        acc_train_array.append(acc_train)\n",
    "        \n",
    "        acc_test = acc_model.score(X_test, y_test)\n",
    "        acc_test_array.append(acc_test)\n",
    "\n",
    "        \n",
    "        #Model 3 - optimal model for FSC\n",
    "        f1_model = LogisticRegression(penalty = opt_f1[\"classifier__penalty\"], C = opt_f1[\"classifier__C\"])\n",
    "        f1_model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_train = f1_model.predict(X_train)\n",
    "        f1_train = f1_score(y_train, y_pred_train)\n",
    "        fsc_train_array.append(f1_train)\n",
    "        \n",
    "        y_pred_test = f1_model.predict(X_test)\n",
    "        f1_test = f1_score(y_test, y_pred_test)\n",
    "        fsc_test_array.append(f1_test)\n",
    "\n",
    "    #take mean of 5 trials for test data\n",
    "    auc_score_train = np.mean(auc_train_array)\n",
    "    acc_score_train = np.mean(acc_train_array)\n",
    "    fsc_score_train = np.mean(fsc_train_array)\n",
    "    \n",
    "    #raw test metrics for each trial\n",
    "    print(auc_test_array)\n",
    "    print(acc_test_array)\n",
    "    print(fsc_test_array)\n",
    "    #take mean of 5 trials for train data\n",
    "    auc_score_test = np.mean(auc_test_array)\n",
    "    acc_score_test = np.mean(acc_test_array)\n",
    "    fsc_score_test = np.mean(fsc_test_array)\n",
    "                                          \n",
    "    #append each dataset's metrics to new list\n",
    "    metrics_test = [auc_score_test, acc_score_test, fsc_score_test]\n",
    "    test_logreg_metrics.append(metrics_test)\n",
    "                             \n",
    "    metrics_train = [auc_score_train, acc_score_train, fsc_score_train]\n",
    "    train_logreg_metrics.append(metrics_train)\n",
    "\n",
    "#4x3 (datasets x metrics) array of optimal errors for each dataset\n",
    "test_logreg_metrics = np.array(test_logreg_metrics)\n",
    "print(\"LR test averages \", test_logreg_metrics)\n",
    "        \n",
    "train_logreg_metrics = np.array(train_logreg_metrics)\n",
    "print(\"LR train averages \", train_logreg_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR test metrics across datasets:  [0.7933824  0.7356245  0.75985558]\n",
      "LR train metrics across datasets:  [0.79624375 0.73759    0.761405  ]\n",
      "LR test metrics across metrics for each algo:  [0.82241064 0.75568573 0.85334832 0.62037195]\n"
     ]
    }
   ],
   "source": [
    "#Table 2 values\n",
    "#train and test metrics averaged across datasets\n",
    "train_logreg_average_datasets = np.mean(train_logreg_metrics, axis = 0)\n",
    "test_logreg_average_datasets = np.mean(test_logreg_metrics, axis = 0)\n",
    "\n",
    "#Table 3 values\n",
    "#test metrics averaged across metrics per dataset\n",
    "test_logreg_average_metrics = np.mean(test_logreg_metrics, axis = 1)\n",
    "\n",
    "\n",
    "print(\"LR test metrics across datasets: \", test_logreg_average_datasets)\n",
    "print(\"LR train metrics across datasets: \", train_logreg_average_datasets)\n",
    "\n",
    "print(\"LR test metrics across metrics for each algo: \", test_logreg_average_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.8525625135916666, 0.8525800626696061, 0.8507332557111714, 0.848885360332594, 0.853027404171069]\n",
      "[0.7994293865905849, 0.8010699001426533, 0.8017831669044223, 0.8005706134094152, 0.7963623395149786]\n",
      "[0.8565204174688418, 0.8594324882818406, 0.8596535528508662, 0.8602419274217735, 0.8574993760918393]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.9897779086127091, 0.9894826791688387, 0.9903844888888889, 0.9921576230082244, 0.9908023597252842]\n",
      "[0.9464, 0.9467333333333333, 0.9522666666666667, 0.949, 0.951]\n",
      "[0.9464214314274291, 0.9465229904290209, 0.9518234423361595, 0.948921679909194, 0.9505416862929817]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.6699599306242484, 0.6598324890972178, 0.6586490135368663, 0.6665472672457539, 0.6624453965053764]\n",
      "[0.6887468030690537, 0.6932225063938618, 0.7047953964194373, 0.6957161125319693, 0.69846547314578]\n",
      "[0.7998684426903471, 0.8055758165167356, 0.8135374177133395, 0.8129502161037299, 0.8099459982268074]\n",
      "Trial  0\n",
      "Trial  1\n",
      "Trial  2\n",
      "Trial  3\n",
      "Trial  4\n",
      "[0.8641116253895453, 0.8618021431541287, 0.861052892399256, 0.8621564223472168, 0.8617561625827835]\n",
      "[0.7820600959702229, 0.7822007180405963, 0.7830461865377805, 0.7817927404290188, 0.782219814864968]\n",
      "[0.7807376977597994, 0.7832801560251588, 0.7856003184221632, 0.7813717508120291, 0.7833219908039147]\n",
      "KNN test averages  [[0.85155772 0.79984308 0.85866955]\n",
      " [0.99052101 0.94908    0.94884625]\n",
      " [0.66348682 0.69618926 0.80837558]\n",
      " [0.86217585 0.78226391 0.78286238]]\n",
      "KNN train averages  [[1.        1.        0.9759745]\n",
      " [1.        1.        1.       ]\n",
      " [1.        1.        1.       ]\n",
      " [1.        1.        1.       ]]\n"
     ]
    }
   ],
   "source": [
    "# empty lists to append metrics per trial\n",
    "test_knn_metrics = []\n",
    "train_knn_metrics = []\n",
    "\n",
    "#looping through all four datasets\n",
    "for X, y in zip(all_X, all_y):\n",
    "    auc_test_array = []\n",
    "    acc_test_array = []\n",
    "    fsc_test_array = []\n",
    "    \n",
    "    auc_train_array = []\n",
    "    acc_train_array = []\n",
    "    fsc_train_array = []\n",
    "    \n",
    "    for trial in range(5):\n",
    "\n",
    "        #splitting into 5000 training samples\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 5000)\n",
    "        \n",
    "        pipe = Pipeline([('std', StandardScaler()),\n",
    "                         ('classifier', KNeighborsClassifier())])\n",
    "\n",
    "        search_space = [{'classifier': [KNeighborsClassifier()],\n",
    "                         'classifier__n_neighbors': [5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57, 61, 65, 69, 73, 77, 81, 85, 89, 93, 97, 101],\n",
    "                        'classifier__metric': [\"euclidean\"],\n",
    "                        'classifier__weights': ['uniform', 'distance']}\n",
    "                       ]\n",
    "\n",
    "        # Create grid search \n",
    "        clf = GridSearchCV(pipe, search_space, cv=StratifiedKFold(n_splits=5), \n",
    "                           scoring=[\"accuracy\",\"roc_auc\", \"f1\"], refit=False)\n",
    "\n",
    "        # Fit grid search\n",
    "        search = clf.fit(X_train, y_train)\n",
    "        \n",
    "\n",
    "        #finding index of the best ranking mean test metrics\n",
    "        auc_rank = search.cv_results_[\"rank_test_roc_auc\"]\n",
    "        acc_rank = search.cv_results_[\"rank_test_accuracy\"]\n",
    "        f1_rank = search.cv_results_[\"rank_test_f1\"]\n",
    "\n",
    "        ind_auc = list(auc_rank).index(min(auc_rank))\n",
    "        ind_acc = list(acc_rank).index(min(acc_rank))\n",
    "        ind_f1 = list(f1_rank).index(min(f1_rank))\n",
    "\n",
    "\n",
    "        #storing optimal parameters as a dictionary\n",
    "        opt_auc = search.cv_results_[\"params\"][ind_auc]\n",
    "        opt_acc = search.cv_results_[\"params\"][ind_acc]\n",
    "        opt_f1 = search.cv_results_[\"params\"][ind_f1]    \n",
    "\n",
    "        print(\"Trial \", trial)\n",
    "        \n",
    "        #Model 1 - optimal model for AUC - ADD CLASSIFIER__ALGORITHM IF YOU ADD IT ABOVE!!!!!! pls\n",
    "        auc_model = KNeighborsClassifier(n_neighbors = opt_auc[\"classifier__n_neighbors\"], metric = opt_auc[\"classifier__metric\"], weights = opt_auc[\"classifier__weights\"])\n",
    "        auc_model.fit(X_train, y_train)\n",
    "        \n",
    "        #predicting on train data\n",
    "        y_pred_train = auc_model.predict_proba(X_train)[:,1]\n",
    "        auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "        auc_train_array.append(auc_train) # appending optimal parameter metric\n",
    "        \n",
    "        #predicting on test data\n",
    "        y_pred_test = auc_model.predict_proba(X_test)[:,1]\n",
    "        auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "        auc_test_array.append(auc_test) # appending optimal parameter metric\n",
    "\n",
    "        \n",
    "        #Model 2 - optimal model for ACC\n",
    "        acc_model = KNeighborsClassifier(n_neighbors = opt_acc[\"classifier__n_neighbors\"], metric = opt_acc[\"classifier__metric\"], weights = opt_acc[\"classifier__weights\"])\n",
    "        acc_model.fit(X_train, y_train)\n",
    "        \n",
    "        #predicting on train data\n",
    "        acc_train = acc_model.score(X_train, y_train)\n",
    "        acc_train_array.append(acc_train)\n",
    "        \n",
    "        #predicting on test data\n",
    "        acc_test = acc_model.score(X_test, y_test)\n",
    "        acc_test_array.append(acc_test)\n",
    "\n",
    "        \n",
    "        #Model 3 - optimal model for FSC\n",
    "        f1_model = KNeighborsClassifier(n_neighbors = opt_f1[\"classifier__n_neighbors\"], metric = opt_f1[\"classifier__metric\"], weights = opt_f1[\"classifier__weights\"])\n",
    "        f1_model.fit(X_train, y_train)\n",
    "        \n",
    "        #predicting on train data\n",
    "        y_pred_train = f1_model.predict(X_train)\n",
    "        f1_train = f1_score(y_train, y_pred_train)\n",
    "        fsc_train_array.append(f1_train)\n",
    "        \n",
    "        #predicting on train data\n",
    "        y_pred_test = f1_model.predict(X_test)\n",
    "        f1_test = f1_score(y_test, y_pred_test)\n",
    "        fsc_test_array.append(f1_test)\n",
    " \n",
    "    #take mean of 5 trials\n",
    "    auc_score_train = np.mean(auc_train_array)\n",
    "    acc_score_train = np.mean(acc_train_array)\n",
    "    fsc_score_train = np.mean(fsc_train_array)\n",
    "    \n",
    "    #print raw test values\n",
    "    print(auc_test_array)\n",
    "    print(acc_test_array)\n",
    "    print(fsc_test_array)\n",
    "    #take mean of 5 trials for train\n",
    "    auc_score_test = np.mean(auc_test_array)\n",
    "    acc_score_test = np.mean(acc_test_array)\n",
    "    fsc_score_test = np.mean(fsc_test_array)\n",
    "    \n",
    "    #append each dataset's metrics to new list\n",
    "    metrics_test = [auc_score_test, acc_score_test, fsc_score_test]\n",
    "    test_knn_metrics.append(metrics_test)\n",
    "                             \n",
    "    metrics_train = [auc_score_train, acc_score_train, fsc_score_train]\n",
    "    train_knn_metrics.append(metrics_train)\n",
    "    \n",
    "#4x3 (datasets x metrics) array of optimal errors for each dataset\n",
    "test_knn_metrics = np.array(test_knn_metrics)\n",
    "print(\"KNN test averages \", test_knn_metrics)\n",
    "        \n",
    "train_knn_metrics = np.array(train_knn_metrics)\n",
    "print(\"KNN train averages \", train_knn_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN test metrics across datasets:  [0.84193535 0.80684406 0.84968844]\n",
      "KNN train metrics across datasets:  [1.         1.         0.99399362]\n",
      "KNN test metrics across metrics for each algo:  [0.83669012 0.96281575 0.72268389 0.80910071]\n"
     ]
    }
   ],
   "source": [
    "#Table 2 values\n",
    "#train and test metrics averaged across datasets\n",
    "train_knn_average_datasets = np.mean(train_knn_metrics, axis = 0)\n",
    "test_knn_average_datasets = np.mean(test_knn_metrics, axis = 0)\n",
    "\n",
    "#Table 3 values\n",
    "#test metrics averaged across metrics per dataset\n",
    "test_knn_average_metrics = np.mean(test_knn_metrics, axis = 1)\n",
    "\n",
    "\n",
    "print(\"KNN test metrics across datasets: \", test_knn_average_datasets)\n",
    "print(\"KNN train metrics across datasets: \", train_knn_average_datasets)\n",
    "\n",
    "print(\"KNN test metrics across metrics for each algo: \", test_knn_average_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "#hardcoding average value arrays to use in t-test function\n",
    "\n",
    "svm_auc = [0.7209479, 0.987907, 0.495545906, 4.77444888]\n",
    "svm_acc = [0.65059914, 0.86672, 0.713785166, 5.12380645]\n",
    "svm_fsc = [0.787375572, 0.846575971, 0.832992003, 0.00000996893356]\n",
    "\n",
    "\n",
    "lr_auc = [0.83023053, 0.81185564, 0.87119401, 0.6602494]\n",
    "lr_acc = [0.78945792, 0.7262, 0.8128133, 0.61402679]\n",
    "lr_fsc = [0.84754345, 0.72900154, 0.87603766, 0.58683967]\n",
    "\n",
    "knn_auc = [0.85155772, 0.99052101, 0.66348682, 0.86217585]\n",
    "knn_acc = [0.79984308, 0.94908, 0.69618926, 0.78226391]\n",
    "knn_fsc = [0.85866955, 0.94884625, 0.80837558, 0.78286238]\n",
    "\n",
    "\n",
    "svm_magic = [0.7238688638821792, 0.719183175311843, 0.7171342435700622, 0.7108442601564061, 0.7271738167202736]\n",
    "svm_letter = [0.9211999046847499, 0.889231783606386, 0.9015471652085436, 0.8849616321673311, 0.9050649294459799]\n",
    "svm_calhous = [0.6812076349902748,0.6814380250539674, 0.6801387107767058,0.6801698469901464, 0.680917573558785]\n",
    "svm_cover = [0.3184924357924284, 0.3214853393457086, 0.3365825019947985, 0.3365577528262426 , 0.3366078057532009]\n",
    "\n",
    "lr_magic = [0.8218320786553274, 0.8216893074020999, 0.8211560616489918, 0.8250910313039236, 0.82228469642375]\n",
    "lr_letter = [0.7560049555120375, 0.75675605211636, 0.7553409040951403, 0.7609349186123765, 0.7493918014451025]\n",
    "lr_calhous = [0.8522271464920994, 0.8487126252047742, 0.8488247598909494,0.864455491112612, 0.8525215962908353]\n",
    "lr_cover = [0.6206692597788723,0.6182496646696012,0.6240549262335505, 0.6159884114791513, 0.6228975114724159]\n",
    "\n",
    "knn_magic = [0.8361707725503645, 0.8376941503647001, 0.8373899918221532, 0.8365659670545943, 0.8356297065926289]\n",
    "knn_letter = [0.9608664466800461,0.9609130009770643,0.964824865963905, 0.9633597676391394,  0.9641146820060887]\n",
    "knn_calhous = [0.7195250587945496, 0.7195436040026051, 0.7256606092232144, 0.725071198627151, 0.7236189559593212]\n",
    "knn_cover = [0.8089698063731893,0.8090943390732946,0.8098997991197332, 0.8084403045294216, 0.8090993227505554]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_means1 = [0.670, 0.686, 0.617]\n",
    "lr_means1 = [0.793, 0.736, 0.759]\n",
    "knn_means1 = [0.842, 0.807, 0.849]\n",
    "\n",
    "svm_means2 = [0.719, 0.900, 0.681, 0.329]\n",
    "lr_means2 = [0.822, 0.756, 0.853, 0.620]\n",
    "knn_means2 = [0.837, 0.963, 0.723, 0.809]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-0.9799351513485244, pvalue=0.3650210803746778)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of one t-test, this process was repeated for each comparison of values in Table 2 and 3\n",
    "stats.ttest_ind(lr_means2, knn_means2, equal_var = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussed with and referenced:\n",
    "- Harmeena Sandhu\n",
    "- Ashna Sood\n",
    "- Urmi Suresh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
